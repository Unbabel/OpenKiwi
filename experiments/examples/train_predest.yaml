### Train Predictor Estimator ###

model: estimator

#### MODEL SPECIFIC OPTS ####

## ESTIMATOR ##

# If load-model points to a pretrained Estimator,
# These settings are ignored.

# LSTM Settings
hidden-est: 125
rnn-layers-est: 1
dropout-est: 0.0
# Use linear layer to reduce dimension prior to LSTM
mlp-est: True

# Multitask Learning Settings #

# Continue training the predictor on the postedited text.
# If set, will do an additional forward pass through the predictor
# Using the SRC, PE pair and add the `Predictor` loss for the tokens in the
# postedited text PE. Recommended if you have access to PE
# Requires setting train-pe, valid-pe
token-level: True
# Predict Sentence Level Scores
# Requires setting train-sentence-scores, valid-sentence-scores
sentence-level: False
# Use probabilistic Loss for sentence scores instead of squared error.
# If set, the model will output mean and variance of a truncated Gaussian
# distribution over the interval [0, 1], and use log-likelihood loss instead
# of mean squared error.
# Seems to improve performance
sentence-ll: False
# Predict Binary Label for each sentence, indicating hter == 0.0
# Requires setting train-sentence-scores, valid-sentence-scores
binary-level: False

# WMT 18 Format Settings #

# Predict target tags. Requires train-target-tags, valid-target-tags to be set.
predict-target: true
target-bad-weight: 2.5
# Predict source tags. Requires train-source-tags, valid-source-tags to be set.
predict-source: false
source-bad-weight: 2.5
# Predict gap tags. Requires train-target-tags, valid-target-tags to be set.
# and wmt18-format set to true
predict-gaps: false
target-bad-weight: 2.5

### GENERAL OPTS ###

# Do not set or set to negative number for CPU
gpu-id: 0

### TRAIN OPTS ###
epochs: 10
# Additionally Eval and checkpoint every n training steps
# Explicitly disable by setting to zero (default)
checkpoint-validation-steps: 300000
# If False, never save the Models
checkpoint-save: true
# Keep Only the n best models according to the main metric (F1Mult by default)
# USeful to avoid filling the harddrive during a long run
checkpoint-keep-only-best: 3
# If greater than zero, Early Stop after n evaluation cycles without improvement
checkpoint-early-stop-patience: 0


# Print Train Stats Every n batches
log-interval: 100
# LR. Currently ADAM is only optimizer supported.
# 1e-3 * (batch_size / 32) seems to work well
learning-rate: 2e-3

train-batch-size: 64
valid-batch-size: 64



### LOADING ###

# Load pretrained (sub-)model.
# If set, the model architecture params are ignored.
# As the vocabulary of the pretrained model will be used,
# all vocab-params will also be ignored.

# (i) load-pred-source or load-pred-target: Predictor instance
#     -> a new Estimator is initialized with the given predictor(s).
# (ii) load-model: Estimator instance.
#                  As the Predictor is a submodule of the Estimator,
#                  load-pred-{source,target} will be ignored if this is set.

# load-model: path_to_estimator
# load-pred-source: path_to_predictor_source_target
#load-pred-target: runs/model.torch


###  DATA ###

# Set to True to use target_tags in WMT18 format
wmt18-format: false

train-source: testdata/WMT17/word_level/train.src
train-target: testdata/WMT17/word_level/train.mt
train-pe: testdata/WMT17/word_level/train.pe
train-target-tags: testdata/WMT17/word_level/train.tags


valid-source: testdata/WMT17/word_level/dev.src
valid-target: testdata/WMT17/word_level/dev.mt
valid-pe: testdata/WMT17/word_level/dev.pe
valid-target-tags: testdata/WMT17/word_level/dev.tags

model: predictor
##### Pretrain Predictor on Parallel Data ######


### LOGGING ###


# Experiment Name for MLFlow
experiment-name: EN-DE Pretrain Predictor
#mlflow-tracking-uri: http://192.168.100.109:5000
# Print Train Stats Every n batches
log-interval: 100
# Pretrain for this many batches
warmup: 0
# Or this many Epochs
epochs: 5


### CHECKPOINTING ###


# Eval and checkpoint every n samples
checkpoint-validation-samples: 0
# If False, never save the Models
checkpoint-save: true
# Keep Only the n best models according to the main metric (Perplexity by default)
# USeful to avoid filling the harddrive during a long run
checkpoint-keep-only-best: 5
# If greater than zero, Early STop after n evaluation cycles without improvement
checkpoint-early-stop-patience: 0
# Directory To save Checkpoints
output-dir: /mnt/data/home/sony/predest_benchmark

### TRAINING ###


# LR. Currently ADAM is only optimizer supported.
learning-rate: 2e-3
train-batch-size: 32
valid-batch-size: 32
# To use CPU, do not set or set to negative number
gpu-id: 0


### MODEL ARCHITECTURE ###


# LSTM Settings (Both SRC and TGT)
hidden-pred: 600
rnn-layers-pred: 1
dropout-pred: 0.0
# If set, takes precedence over other embedding params
embedding-sizes: 300
# Source, Target, and Target Softmax Embedding
source-embeddings-size: 300
target-embeddings-size: 300
embed-out: 300
# Set to true to predict from source to target
predict-inverse: false


### DATA ###


# Source and Target Files
train-source: /mnt/data/home/sony/benchmark/train.en
train-target: /mnt/data/home/sony/benchmark/train.de
# Optionally load more data which is used only for vocabulary creation.
# This is useful to reduce OOV words if the parallel data
# and QE data are from different domains.
#extend-source-vocab: /mnt/data/home/sony/PredEst/data/ov/500K.train.src
#extend-target-vocab: /mnt/data/home/sony/PredEst/data/ov/500K.train.pe
# Optionally Specify Validation Sets
valid-source: /mnt/data/home/sony/benchmark/val.en
valid-target: /mnt/data/home/sony/benchmark/val.de
# If No valid is specified, randomly split the train corpus
#split: 0.99


### VOCAB ###

# Load vocab file from a previous run.
# This takes precedence over the other vocab options.
source-vocab-size: 70000
target-vocab-size: 70000
# Remove Sentences not in the specified Length Range
source-max-length: 80
source-min-length: 1
target-max-length: 80
target-min-length: 1
# Require Minimum Frequency of words
source-vocab-min-frequency: 1
target-vocab-min-frequency: 1

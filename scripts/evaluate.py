import argparse
import os.path
import sys
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
from more_itertools import flatten
from scipy.stats.stats import pearsonr, rankdata, spearmanr

from kiwi import constants
from kiwi.cli.opts import PathType
from kiwi.data.utils import read_file
from kiwi.metrics.functions import (
    delta_average,
    f1_scores,
    mean_absolute_error,
    mean_squared_error,
)
from kiwi.metrics.metrics import MovingF1
from kiwi.metrics.metrics import MovingSkipsAtQuality as SkipsAtQ


def parse_args():
    parser = argparse.ArgumentParser(prog='Evaluate WMT Quality Estimation')

    # Options
    parser.add_argument(
        '--type',
        help='Input type for prediction file',
        choices=['probs', 'tags'],
        type=str,
        default='probs',
    )
    parser.add_argument(
        '--format',
        help='Input format for gold files',
        choices=['wmt17', 'wmt18'],
        type=str,
        default='wmt17',
    )
    parser.add_argument(
        '--pred-format',
        help='Input format for predicted files. Defaults to the same as '
        '--format.',
        choices=['wmt17', 'wmt18'],
        type=str,
        default=None,
    )
    parser.add_argument(
        '--sents-avg',
        help='Obtain scores for sentences by averaging over tags or '
        'probabilities.',
        choices=['probs', 'tags'],
        type=str,
        # default=None
    )

    # Gold files.
    parser.add_argument(
        '--gold-sents',
        help='Sentences gold standard. ',
        type=PathType(exists=True),
        required=False,
    )
    parser.add_argument(
        '--gold-target',
        help='Target tags gold standard, or target and gaps '
        'if format == "wmt18".',
        type=PathType(exists=True),
        required=False,
    )
    parser.add_argument(
        '--gold-source',
        help='Source tags gold standard.',
        type=PathType(exists=True),
        required=False,
    )
    parser.add_argument(
        '--gold-cal',
        help='Target Tags to calibrate.',
        type=PathType(exists=True),
        required=False,
    )

    # Prediction Files
    parser.add_argument(
        '--input-dir',
        help='Directory with prediction files generated by predict pipeline. '
        'Setting this argument will evaluate all predictions for '
        'which a gold file is set.',
        nargs='+',
        type=PathType(exists=True),
        # required=True
    )
    parser.add_argument(
        '--pred-sents',
        help='Sentences HTER predictions.',
        type=PathType(exists=True),
        nargs='+',
        required=False,
    )
    parser.add_argument(
        '--pred-target',
        help='Target predictions; can be tags or probabilities (of BAD). '
        'See --type.',
        type=PathType(exists=True),
        nargs='+',
        required=False,
    )
    parser.add_argument(
        '--pred-gaps',
        help='Gap predictions; can be tags or probabilities (of BAD). '
        '(see --type). Use this option for files that only contain gap '
        'tags.',
        type=PathType(exists=True),
        nargs='+',
        required=False,
    )
    parser.add_argument(
        '--pred-source',
        help='Source predictions. can be tags or probabilities (of BAD). '
        ' See --type.',
        type=PathType(exists=True),
        nargs='+',
        required=False,
    )
    parser.add_argument(
        '--pred-cal',
        help='Target Predictions to calibrate.',
        type=PathType(exists=True),
        required=False,
    )

    return parser.parse_args()


def probs_to_labels(probs, threshold=0.5):
    """Generates numeric labels from probabilities.

    This assumes two classes and default decision threshold 0.5

    """
    return [int(float(prob) > threshold) for prob in probs]


def average(probs_per_file):
    # flat_probs = [list(flatten(probs)) for probs in probs_per_file]
    probabilities = np.array(probs_per_file, dtype='float32')
    return probabilities.mean(axis=0).tolist()


# def majority(tags_per_file):
#     from collections import Counter
#
#     voted_sentences = []
#     for parallel_sentences in zip(*tags_per_file):
#         voted_sentences.append([
#             Counter(parallel_tokens).most_common(1)[0][0]
#             for parallel_tokens in zip(*parallel_sentences)])
#     return voted_sentences


def read_sentence_scores(sent_file):
    """Read File with numeric scores for sentecnes.
    """
    return np.array([line.strip() for line in open(sent_file)], dtype='float')


def wmt_to_labels(corpus):
    """Generates numeric labels from text labels."""
    dictionary = dict(zip(constants.LABELS, range(len(constants.LABELS))))
    return [[dictionary[word] for word in sent] for sent in corpus]


def tags_to_numerical(tags):
    return [constants.LABELS.index(tag) for tag in tags]


def split_wmt18(tags):
    """Split tags list of lists in WMT18 format into target and gap tags."""
    tags_mt = [sent_tags[1::2] for sent_tags in tags]
    tags_gaps = [sent_tags[::2] for sent_tags in tags]
    return tags_mt, tags_gaps


def score_word_level(gold, prediction, threshold=0.5):
    gold_tags = gold
    pred_tags = probs_to_labels(prediction, threshold=threshold)
    return f1_scores(pred_tags, gold_tags)


def tags_to_sentence_score(tags_sentences):
    scores = []
    bad_label = constants.LABELS.index(constants.BAD)
    for tags in tags_sentences:
        labels = probs_to_labels(tags)
        scores.append(labels.count(bad_label) / len(tags))
    return scores


def probs_to_sentence_score(probs_sentences):
    scores = []
    for probs in probs_sentences:
        probs = [float(p) for p in probs]
        scores.append(np.mean(probs))
    return scores


def score_sentence_level(gold, pred):
    pearson = pearsonr(gold, pred)
    mae = mean_absolute_error(gold, pred)
    rmse = np.sqrt(mean_squared_error(gold, pred))

    spearman = spearmanr(
        rankdata(gold, method='ordinal'), rankdata(pred, method='ordinal')
    )
    delta_avg = delta_average(gold, rankdata(pred, method='ordinal'))

    return (pearson[0], mae, rmse), (spearman[0], delta_avg)


def extract_path_prefix(file_names):
    if len(file_names) < 2:
        return '', file_names
    prefix_path = os.path.commonpath(
        [path for path in file_names if not path.startswith('*')]
    )
    if len(prefix_path) > 0:
        file_names = [
            os.path.relpath(path, prefix_path)
            if not path.startswith('*')
            else path
            for path in file_names
        ]
    return prefix_path, file_names


def print_scores_table(scores, prefix='TARGET'):
    scoring = np.array(
        scores,
        dtype=[
            ('File', 'object'),
            ('F1_{}'.format(constants.LABELS[0]), float),
            ('F1_{}'.format(constants.LABELS[1]), float),
            ('xF1', float),
        ],
    )

    # Put the main metric in the first column
    scoring = scoring[
        [
            'File',
            'xF1',
            'F1_{}'.format(constants.LABELS[0]),
            'F1_{}'.format(constants.LABELS[1]),
        ]
    ]

    prefix_path, scoring['File'] = extract_path_prefix(scoring['File'])
    path_str = ' ({})'.format(prefix_path) if prefix_path else ''

    max_method_length = max(len(path_str) + 4, max(map(len, scoring['File'])))
    print('-' * (max_method_length + 13 * 3))
    print('Word-level scores for {}:'.format(prefix))
    print(
        '{:{width}}    {:9}    {:9}    {:9}'.format(
            'File{}'.format(path_str),
            'xF1',
            'F1_{}'.format(constants.LABELS[0]),
            'F1_{}'.format(constants.LABELS[1]),
            width=max_method_length,
        )
    )
    for score in np.sort(scoring, order=['xF1', 'File'])[::-1]:
        print(
            '{:{width}s}    {:<9.5f}    {:<9.5}    {:<9.5f}'.format(
                *score, width=max_method_length
            )
        )


def print_sentences_scoring_table(scores):
    scoring = np.array(
        scores,
        dtype=[
            ('File', 'object'),
            ('Pearson r', float),
            ('MAE', float),
            ('RMSE', float),
        ],
    )
    prefix_path, scoring['File'] = extract_path_prefix(scoring['File'])
    path_str = ' ({})'.format(prefix_path) if prefix_path else ''

    max_method_length = max(len(path_str) + 4, max(map(len, scoring['File'])))
    print('-' * (max_method_length + 13 * 3))
    print('Sentence-level scoring:')
    print(
        '{:{width}}    {:9}    {:9}    {:9}'.format(
            'File{}'.format(path_str),
            'Pearson r',
            'MAE',
            'RMSE',
            width=max_method_length,
        )
    )
    for score in np.sort(scoring, order=['Pearson r', 'File'])[::-1]:
        print(
            '{:{width}s}    {:<9.5f}    {:<9.5f}    {:<9.5f}'.format(
                *score, width=max_method_length
            )
        )


def print_sentences_ranking_table(scores):
    scoring = np.array(
        scores,
        dtype=[('File', 'object'), ('Spearman r', float), ('DeltaAvg', float)],
    )
    prefix_path, scoring['File'] = extract_path_prefix(scoring['File'])
    path_str = ' ({})'.format(prefix_path) if prefix_path else ''

    max_method_length = max(len(path_str) + 4, max(map(len, scoring['File'])))
    print('-' * (max_method_length + 13 * 3))
    print('Sentence-level ranking:')
    print(
        '{:{width}}    {:10}    {:9}'.format(
            'File{}'.format(path_str),
            'Spearman r',
            'DeltaAvg',
            width=max_method_length,
        )
    )  # noqa
    for score in np.sort(scoring, order=['Spearman r', 'File'])[::-1]:
        print(
            '{:{width}s}    {:<10.5f}    {:<9.5f}'.format(
                *score, width=max_method_length
            )
        )


def check_lengths(gold, prediction):
    for i, (g, p) in enumerate(zip(gold, prediction)):
        if len(g) != len(p):
            warnings.warn(
                'Mismatch length for {}th sample '
                '{} x {}'.format(i, len(g), len(p))
            )


def eval_word_level(golds, pred_files, tag_name, threshold=None):
    scores_table = []
    for pred_file, pred in pred_files[tag_name]:
        check_lengths(golds[tag_name], pred)

        scores = score_word_level(
            list(flatten(golds[tag_name])), list(flatten(pred))
        )

        scores_table.append((pred_file, *scores))

        if threshold is not None:
            scores_cal = score_word_level(
                list(flatten(golds[tag_name])),
                list(flatten(pred)),
                threshold=threshold,
            )
            scores_table.append(('CAL' + pred_file, *scores_cal))
    # If more than one system is provided, compute ensemble score
    if len(pred_files[tag_name]) > 1:
        ensemble_pred = average(
            [list(flatten(pred)) for _, pred in pred_files[tag_name]]
        )
        ensemble_score = score_word_level(
            list(flatten(golds[tag_name])), ensemble_pred
        )
        scores_table.append(('*ensemble*', *ensemble_score))

    print_scores_table(scores_table, tag_name)


def eval_sentence_level(sent_gold, sent_preds):
    sentence_scores, sentence_ranking = [], []
    for file_name, pred in sent_preds:
        scoring, ranking = score_sentence_level(sent_gold, pred)

        sentence_scores.append((file_name, *scoring))
        sentence_ranking.append((file_name, *ranking))

    ensemble_pred = average([pred for _, pred in sent_preds])
    ensemble_score, ensemble_ranking = score_sentence_level(
        sent_gold, ensemble_pred
    )
    sentence_scores.append(('*ensemble*', *ensemble_score))
    sentence_ranking.append(('*ensemble*', *ensemble_ranking))

    print_sentences_scoring_table(sentence_scores)
    print_sentences_ranking_table(sentence_ranking)


def calibrate_threshold(scores, golds):
    m = MovingF1()
    scores = [float(x) for x in flatten(scores)]
    golds = list(flatten(wmt_to_labels(golds)))
    f1, threshold = m.choose(m.eval(scores, golds))
    print('xF1 calibrate: {}'.format(f1))
    return threshold


def eval_skips_at_quality(
    sent_labels,
    sent_scores,
    target=0.0,
    scores_higher_is_better=False,
    labels_higher_is_better=True,
):
    m = SkipsAtQ(
        scores_higher_is_better=scores_higher_is_better,
        labels_higher_is_better=labels_higher_is_better,
    )
    m_oracle = SkipsAtQ(
        scores_higher_is_better=labels_higher_is_better,
        labels_higher_is_better=labels_higher_is_better,
    )
    skips_at_q = {}
    oracle_graph, _ = zip(*m_oracle.eval(sent_labels, sent_labels))
    skips_at_q['oracle'] = oracle_graph
    for pred_file, scores in sent_scores:
        graph, _ = zip(*m.eval(scores, sent_labels))
        skips_at_q[pred_file] = graph
    print_graphs(skips_at_q, '.')


def print_graphs(graphs, output_dir):
    import seaborn as sns

    df_dict = {'source': [], 'skips': [], 'ter': []}
    for name, graph in graphs.items():
        skips, qual = zip(*graph)
        df_dict['skips'] += skips
        df_dict['ter'] += qual
        df_dict['source'] += len(skips) * [name]
    sns.set()
    df = pd.DataFrame(df_dict)
    plot = sns.lineplot(
        x='skips', y='ter', hue='source', style='source', data=df
    )
    plot.figure.savefig(str(Path(output_dir) / 'SkipsAtQ.png'))


def main(args):
    wmt18_format = args.format.lower() == 'wmt18'
    if args.pred_format is None:
        wmt18_pred_format = wmt18_format
    else:
        wmt18_pred_format = args.pred_format.lower() == 'wmt18'

    golds = {}
    if args.gold_target:
        gold_target = wmt_to_labels(read_file(args.gold_target))
        if wmt18_format:
            gold_target, gold_gaps = split_wmt18(gold_target)
            golds[constants.GAP_TAGS] = gold_gaps
        golds[constants.TARGET_TAGS] = gold_target
    if args.gold_source:
        gold_source = wmt_to_labels(read_file(args.gold_source))
        golds[constants.SOURCE_TAGS] = gold_source
    if args.gold_sents:
        gold_sentences = read_sentence_scores(args.gold_sents)
        golds[constants.SENTENCE_SCORES] = gold_sentences

    pred_files = {target: [] for target in constants.TARGETS}
    if args.pred_target:
        for pred_file in args.pred_target:
            pred_target = read_file(pred_file)
            if wmt18_pred_format:
                pred_target, pred_gaps = split_wmt18(pred_target)
                pred_files[constants.GAP_TAGS].append(
                    (str(pred_file), pred_gaps)
                )
            pred_files[constants.TARGET_TAGS].append(
                (str(pred_file), pred_target)
            )
    if args.pred_gaps:
        for pred_file in args.pred_gaps:
            pred_gaps = read_file(pred_file)
            pred_files[constants.GAP_TAGS].append((str(pred_file), pred_gaps))
    if args.pred_source:
        for pred_file in args.pred_source:
            pred_source = read_file(pred_file)
            pred_files[constants.SOURCE_TAGS].append(
                (str(pred_file), pred_source)
            )
    if args.pred_sents:
        for pred_file in args.pred_sents:
            pred_sents = read_sentence_scores(pred_file)
            pred_files[constants.SENTENCE_SCORES].append(
                (str(pred_file), pred_sents)
            )

    if args.input_dir:
        for input_dir in args.input_dir:
            input_dir = Path(input_dir)
            for target in constants.TAGS:
                pred_file = input_dir.joinpath(target)
                if pred_file.exists() and pred_file.is_file():
                    pred_files[pred_file.name].append(
                        (str(pred_file), read_file(pred_file))
                    )
            for target in [constants.SENTENCE_SCORES, constants.BINARY]:
                pred_file = input_dir.joinpath(target)
                if pred_file.exists() and pred_file.is_file():
                    pred_files[pred_file.name].append(
                        (str(pred_file), read_sentence_scores(str(pred_file)))
                    )

    threshold = None
    if args.pred_cal and args.gold_cal and args.type == 'probs':
        scores_cal = read_file(args.pred_cal)
        golds_cal = read_file(args.gold_cal)
        if wmt18_format:
            golds_cal, _ = split_wmt18(golds_cal)
        if wmt18_pred_format:
            scores_cal, _ = split_wmt18(scores_cal)
        threshold = calibrate_threshold(scores_cal, golds_cal)

    # Numericalize Text Labels
    if args.type == 'tags':
        for tag_name in constants.TAGS:
            for i in range(len(pred_files[tag_name])):
                fname, pred_tags = pred_files[tag_name][i]
                pred_files[tag_name][i] = (fname, wmt_to_labels(pred_tags))

    if not any(pred_files.values()):
        print(
            'Please specify at least one of these options: '
            '--input-dir, --pred-target, --pred-source, --pred-sents'
        )
        return
    for tag in constants.TAGS:
        if tag in golds and pred_files[tag]:
            t = threshold if tag == constants.TARGET_TAGS else None
            eval_word_level(golds, pred_files, tag, threshold=t)

    if constants.SENTENCE_SCORES in golds:
        sent_golds = golds[constants.SENTENCE_SCORES]
        sent_preds = pred_files[constants.SENTENCE_SCORES]
        sents_avg = args.sents_avg if args.sents_avg else args.type
        tag_to_sent = probs_to_sentence_score

        if sents_avg == 'tags':
            tag_to_sent = tags_to_sentence_score

        for pred_file, pred in pred_files[constants.TARGET_TAGS]:
            sent_pred = np.array(tag_to_sent(pred))
            sent_preds.append((pred_file, sent_pred))

        if sent_preds:
            eval_sentence_level(sent_golds, sent_preds)

        for pred_file in pred_files[constants.BINARY]:
            sent_preds.append(pred_file)
        if sent_preds:
            eval_skips_at_quality(sent_golds, sent_preds)


if __name__ == '__main__':
    print(' '.join(sys.argv))
    args = parse_args()
    main(args)
